# GPT from Scratch ðŸš€

A from-scratch implementation of a GPT-like transformer model for character-level language modeling, inspired by Andrej Karpathy's "Let's build GPT" tutorial.

## Features
- Pure PyTorch implementation
- Character-level tokenization
- Multi-head self-attention
- Transformer blocks with residual connections
- Text generation capabilities

## Results
Trained on Shakespeare's works:
- Final training loss: 1.0714
- Validation loss: 1.4883
